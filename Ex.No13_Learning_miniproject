# Ex.No: 10 Learning – Use Supervised Learning  
### DATE: 1.11.2024                                                                         
### REGISTER NUMBER : 212222060263 
### AIM: 
To write a program to predict chess game winning using gradient boosting.
###  Algorithm:
1.. Data Collection and Preparation:
•	Collect Data: Gather a dataset of chess games that includes features such as number of moves, piece count difference, average time per move, rating difference between players, and the game outcome (win/loss).
•	Data Cleaning: Handle any missing or inconsistent data. Ensure all data is in a suitable format for analysis.
•	Feature Engineering: Create relevant features from the raw data that can help in predicting the outcome of the game.
2.  Data Preprocessing:
•	Split Data: Divide the dataset into training and testing sets. A common split is 80% for training and 20% for testing.
•	Normalization: Normalize or standardize the features if necessary to ensure that all features contribute equally to the prediction.
3.  Model Selection and Training:
•	Choose Model: Select the Gradient Boosting algorithm (e.g., GradientBoostingClassifier from scikit-learn).
•	Hyperparameter Tuning: Set initial hyperparameters for the Gradient Boosting model, such as the number of estimators, learning rate, and maximum depth of trees.
•	Train Model: Fit the model to the training data using the selected hyperparameters.
 4. Model Evaluation:
•	Make Predictions: Use the trained model to make predictions on the testing data.
•	Evaluate Performance: Assess the model’s performance using metrics such as accuracy, precision, recall, and F1 score. Generate a classification report to summarize these metrics.
5.  Model Optimization:
•	Hyperparameter Optimization: Use techniques such as Grid Search or Random Search to find the best combination of hyperparameters.
•	Cross-Validation: Apply cross-validation to ensure the model’s robustness and prevent overfitting.
6.  Model Deployment:
•	Final Model Training: Train the final model using the entire dataset with the optimized hyperparameters.
•	Save Model: Save the trained model for future use in making predictions on new data.
7.Prediction on New Data:
•	Load Model: Load the saved model.
•	Preprocess New Data: Prepare new data in the same format as the training data.
•	Make Predictions: Use the model to predict the outcome of new chess games.

### Program:
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.ensemble import GradientBoostingClassifier

from sklearn.metrics import accuracy_score, classification_report

data = {

    'num_moves': [40, 55, 38, 60, 45, 70, 52, 36, 48, 51],

    'piece_count_diff': [2, -3, 1, -1, 0, 5, -2, 1, 3, -4],

    'avg_time_per_move': [15.2, 12.5, 20.1, 14.8, 18.3, 10.5, 16.7, 17.8, 13.4, 19.2],

    'rating_diff': [200, -150, 100, -50, 0, 250, -100, 150, -200, 50],

    'winner': [1, 0, 1, 0, 1, 1, 0, 1, 0, 0]

}

df = pd.DataFrame(data)

X = df.drop('winner', axis=1)

y = df['winner']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')

print('Classification Report:')

print(report)


### Output:
Accuracy: 0.5
Classification Report:
              precision    recall  f1-score   support

           0       1.00      0.50      0.67         2
           1       0.00      0.00      0.00         0

    accuracy                           0.50         2
   macro avg       0.50      0.25      0.33         2
weighted avg       1.00      0.50      0.67         2


### Result:
Thus the chess win prediction was excuted.
